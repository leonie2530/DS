{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcwDDyNdmQh9CeYFNIG0k6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonie2530/DS/blob/master/NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZoxAOlrj7fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLmuC4ysYBs_",
        "colab_type": "code",
        "outputId": "990955df-4426-472c-c117-021bf052e9d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oLFxFQuYfLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\" This is my first project in Data Science and i am excited about it. I lack lot of skills and i am trying to develop them! Hoping for the best\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhTiG9UZbPAd",
        "colab_type": "code",
        "outputId": "71054520-a18c-488e-b3b0-67ad89cbb434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(text)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " This is my first project in Data Science and i am excited about it. I lack lot of skills and i am trying to develop them! Hoping for the best\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJrDPXyEbZyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR5MbFqnbrcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenised_sentences = sent_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1bRnowcSXx",
        "colab_type": "code",
        "outputId": "a99e9ea5-a621-40f1-f4a2-6421da4e8559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tokenised_sentences)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' This is my first project in Data Science and i am excited about it.', 'I lack lot of skills and i am trying to develop them!', 'Hoping for the best']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI9-l8RrcA-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J66Siy5dcjYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_words = word_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Tu27JMc_U7",
        "colab_type": "code",
        "outputId": "bc427813-84e6-4f05-e15a-2892ecc51cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tokenized_words)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'my', 'first', 'project', 'in', 'Data', 'Science', 'and', 'i', 'am', 'excited', 'about', 'it', '.', 'I', 'lack', 'lot', 'of', 'skills', 'and', 'i', 'am', 'trying', 'to', 'develop', 'them', '!', 'Hoping', 'for', 'the', 'best']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aebWFx_xdL2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO4yKz-kdYDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dytG8wGA-vte",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "24123909-4f5b-4036-cf51-8fa068fac940"
      },
      "source": [
        "\n",
        "print(stop_words)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'myself', 'of', \"you've\", 'doesn', 'further', 'about', 'whom', 'against', 'again', 'other', \"hadn't\", 'in', \"couldn't\", 'wasn', \"won't\", \"you're\", 'a', 'when', 'not', 'aren', 'her', \"shan't\", 'now', 'how', \"it's\", 'to', 'yourself', 'our', 'which', 'over', 'because', 'own', 'too', 'if', \"you'd\", 'their', \"shouldn't\", 'y', 'why', 'can', 've', \"aren't\", \"weren't\", \"mustn't\", 'during', 'more', 'will', \"haven't\", 'isn', 'he', 'them', 'do', 'so', 'weren', 'your', 'ourselves', \"doesn't\", 'that', 'ours', 'you', 'himself', 'through', 'herself', 'off', 'where', 're', 'itself', 'mightn', 'is', 'below', 'these', \"needn't\", 'each', 'those', 'both', \"hasn't\", 't', 'are', 'under', 'don', 'what', 'into', 'were', 'and', 'have', 'up', 'me', 'ma', 'it', 'for', \"should've\", 'with', 'shan', 'mustn', 'his', 'didn', 'nor', 'the', 'some', 'there', 'themselves', 'hadn', 'out', 'above', 'was', 'being', 'then', 'am', 'down', 'while', 'my', \"wasn't\", 'all', 'this', 's', 'they', 'ain', 'doing', 'she', \"don't\", 'at', 'theirs', 'once', \"she's\", 'as', 'yourselves', 'be', 'its', \"isn't\", 'does', \"mightn't\", 'before', 'did', 'has', 'but', 'had', 'very', 'haven', 'same', \"didn't\", 'just', 'wouldn', 'than', 'couldn', 'hasn', 'after', 'until', 'should', 'hers', 'm', \"that'll\", 'from', 'on', 'most', 'd', 'here', 'few', 'by', 'll', 'who', 'yours', 'any', 'no', 'shouldn', 'needn', 'o', 'an', 'between', 'such', \"wouldn't\", 'only', 'him', 'we', \"you'll\", 'won', 'been', 'i', 'having', 'or'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izp_Vx1w-yAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_words = []\n",
        "for word in tokenized_words :\n",
        "    if word not in stop_words :\n",
        "        filtered_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFqfxtCW-4fK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "805a42e3-44cc-4d8f-9559-b0e8b230701a"
      },
      "source": [
        "\n",
        "print(filtered_words)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'first', 'project', 'Data', 'Science', 'excited', '.', 'I', 'lack', 'lot', 'skills', 'trying', 'develop', '!', 'Hoping', 'best']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI_XRd8rfg5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ9NmxuXf6jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Sstemmer = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXE0Cqlnf9IF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_words = []\n",
        "for word in filtered_words :\n",
        "    stemmed_words.append(Sstemmer.stem(word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMGRpwTU_Acj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43d9cef2-b792-446f-fbd6-f4d633667b5e"
      },
      "source": [
        "print(stemmed_words)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'first', 'project', 'data', 'scienc', 'excit', '.', 'I', 'lack', 'lot', 'skill', 'tri', 'develop', '!', 'hope', 'best']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5lh526T_E7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O039IMhG_Iea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FDc0uim_Ltq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "lemmatized_words = []\n",
        "for word in filtered_words :\n",
        "    lemmatized_words.append(lemmatizer.lemmatize(word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K6MXeJA_Mio",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a54ef7f2-4b0b-4482-e94c-d673d43b7ebd"
      },
      "source": [
        "\n",
        "print(lemmatized_words)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'first', 'project', 'Data', 'Science', 'excited', '.', 'I', 'lack', 'lot', 'skill', 'trying', 'develop', '!', 'Hoping', 'best']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CNm6aSt_PaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_tokens = nltk.pos_tag(tokenized_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sFkz5iz_Si3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6dcccb36-bbff-454e-dd65-57fc90726821"
      },
      "source": [
        "print(pos_tokens)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('my', 'PRP$'), ('first', 'JJ'), ('project', 'NN'), ('in', 'IN'), ('Data', 'NNP'), ('Science', 'NNP'), ('and', 'CC'), ('i', 'VB'), ('am', 'VBP'), ('excited', 'VBN'), ('about', 'IN'), ('it', 'PRP'), ('.', '.'), ('I', 'PRP'), ('lack', 'VBP'), ('lot', 'NN'), ('of', 'IN'), ('skills', 'NNS'), ('and', 'CC'), ('i', 'NN'), ('am', 'VBP'), ('trying', 'VBG'), ('to', 'TO'), ('develop', 'VB'), ('them', 'PRP'), ('!', '.'), ('Hoping', 'VBG'), ('for', 'IN'), ('the', 'DT'), ('best', 'JJS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGIZ3bn8_3cY",
        "colab_type": "text"
      },
      "source": [
        "Bag of Words Model\n",
        "Machine learning algorithms cannot work with raw data directly; the data  must be converted into numbers. Specifically, vectors of numbers. The vectors x are derived from textual data, in order to reflect various linguistic properties of the data. This is called feature extraction or feature encoding.\n",
        "\n",
        "The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms.\n",
        "In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
        "The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "1) A vocabulary of known words.\n",
        "2) A measure of the presence of known words.\n",
        "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded.\n",
        "The model is only concerned with whether known words occur in the document, not where in the document.The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document.\n",
        "Steps\n",
        "\n",
        "We iterate through each sentence in the corpus(paragraph), convert the sentence to lower case, and then remove the punctuation and empty spaces from the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QGA3M4R_vvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Import Regular Expression module\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_9lKN3dBdyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "c741436f-0076-4f24-a27d-61783d8a2fee"
      },
      "source": [
        "corpus = tokenized_sentences"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-088bd28d122a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenized_sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrbr5wAN_yfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "6d4685ed-e9bf-4a97-b88b-c14f3cdd8abb"
      },
      "source": [
        "for i in range(len(corpus)):\n",
        "    corpus[i] = corpus[i].lower()\n",
        "    corpus[i] = re.sub(r'\\W',' ',corpus[i])\n",
        "    corpus[i] = re.sub(r'\\s+',' ',corpus[i])"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-fe4c05488a2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\W'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Y6SGQpALI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxNh2fgOAP5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordfreq = {}\n",
        "for sentence in corpus:\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    for token in tokens:\n",
        "        if token not in wordfreq.keys():\n",
        "            wordfreq[token] = 1\n",
        "        else:\n",
        "            wordfreq[token] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqi8ndDyATSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordfreq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzeQDdRDAZZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_vectors = []\n",
        "for sentence in corpus:\n",
        "    sentence_tokens = nltk.word_tokenize(sentence)\n",
        "    sent_vec = []\n",
        "    for token in wordfreq:\n",
        "        if token in sentence_tokens:\n",
        "            sent_vec.append(1)\n",
        "        else:\n",
        "            sent_vec.append(0)\n",
        "    sentence_vectors.append(sent_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBo3sg_tAaA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dp26Y14AcQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sentence_vector_array = np.asarray(sentence_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyGBYsEKAezB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_vector_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNpLteUsAkQj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "TF-IDF Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Brq9S4WAhVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_idf_values = {}\n",
        "for token in wordfreq:\n",
        "    doc_containing_word = 0\n",
        "    for document in corpus:\n",
        "        if token in nltk.word_tokenize(document):\n",
        "            doc_containing_word += 1\n",
        "    word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3pwGf7TAr5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "word_idf_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE7TDp1-Axi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_tf_values = {}\n",
        "for token in wordfreq:\n",
        "    sent_tf_vector = []\n",
        "    for document in corpus:\n",
        "        doc_freq = 0\n",
        "        for word in nltk.word_tokenize(document):\n",
        "            if token == word:\n",
        "                  doc_freq += 1\n",
        "        word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
        "        sent_tf_vector.append(word_tf)\n",
        "    word_tf_values[token] = sent_tf_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pf9KxYOAyMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "word_tf_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFFUmFpiA0zH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_values = []\n",
        "for token in word_tf_values.keys():\n",
        "    tfidf_sentences = []\n",
        "    for tf_sentence in word_tf_values[token]:\n",
        "        tf_idf_score = tf_sentence * word_idf_values[token]\n",
        "        tfidf_sentences.append(tf_idf_score)\n",
        "    tfidf_values.append(tfidf_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0OIP0iRA4IA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_model = np.asarray(tfidf_values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMvPe0t8A6Il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tf_idf_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YZCVOZtA8VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_model_t = np.transpose(tf_idf_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU_OC7EkA_gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tf_idf_model_t"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}